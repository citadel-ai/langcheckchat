{
    "what is langcheck": {
        "response_message": "LangCheck is a Pythonic toolkit that can be used to evaluate LLM applications and create unit tests, monitoring, guardrails, and more. It can evaluate text produced by any LLM and any library, and its output can be printed as a DataFrame or visualized in an interactive chart. LangCheck is designed as a library of building blocks that can be adapted for various use cases.",
        "source": "LangCheck Documentation\n\n\n\n\nContents\n\n  * Indices\n\n\n\n\nLangCheck Documentation\n\nLangCheck is a simple, Pythonic toolkit to evaluate LLM applications \u2013 use it\nto create unit tests, monitoring, guardrails, and more.\n\nGet started with the docs below.\n\n  * Installation\n  * Quickstart\n  * Metrics\n  * Tutorials\n  * API Reference\n  * Contributing\n  * GitHub\n\n\n\n\nIndices\n\n  * Index\n\n  * Module Index\n\n[\n\nnext\n\nInstallation\n\n__](installation.html \"next page\")\n\n__Contents\n\n  * Indices\n\nBy Citadel AI\n\n\u00a9 Copyright 2023, Citadel AI.\nQuickstart\n\n\n\n\nContents\n\n  * Using LangCheck\n  * Use Cases\n    * Unit Testing\n    * Monitoring\n    * Guardrails\n\n\n\n\nQuickstart\n\n\n\n\nUsing LangCheck\n\nTip\n\nLangCheck runs anywhere, but its built-in visualizations look best in a\nnotebook (e.g. Jupyter, [VS\nCode](https://code.visualstudio.com/docs/datascience/jupyter-notebooks),\nColab). [Try this quickstart in\nColab](https://colab.research.google.com/github/citadel-\nai/langcheck/blob/main/docs/notebooks/LangCheck%20Quickstart.ipynb).\n\nLangCheck evaluates text produced by an LLM.\n\nThe input to LangCheck is just a list of strings, so it works with any LLM &\nany library. For example:\n\n    \n    \n    import langcheck\n    \n    # Generate text with any LLM library\n    generated_outputs = [\n        'Black cat the',\n        'The black cat is.',\n        'The black cat is sitting',\n        'The big black cat is sitting on the fence',\n        'Usually, the big black cat is sitting on the old wooden fence.'\n    ]\n    \n    # Check text quality and get results as a DataFrame\n    langcheck.metrics.fluency(generated_outputs)\n    \n\nThe output of\n[`langcheck.metrics.fluency()`](langcheck.metrics.html#langcheck.metrics.fluency\n\"langcheck.metrics.fluency\") (and any metric function) can be\nprinted as a DataFrame:\n\n!MetricValue output\n\nIt\u2019s more than just a DataFrame, though. Try setting a threshold to view\npass/fail results:\n\n    \n    \n    fluency_values = langcheck.metrics.fluency(generated_outputs)\n    fluency_values > 0.5\n    \n\n!MetricValue output\n\nYou can also set an assertion (useful in unit tests!):\n\n    \n    \n    assert fluency_values > 0.5\n    \n\nAnd quickly visualize the results in an interactive chart:\n\n    \n    \n    fluency_values.scatter()\n    \n\n!Scatter plot for one metric\n\nTo get the underlying DataFrame for custom analysis, just call `to_df()`:\n\n    \n    \n    fluency_values.to_df()\n    (fluency_values > 0.5).to_df()\n    \n\nFinally, metric functions can also take a single string as input, which is\nuseful for monitoring and guardrails use cases.\n\n    \n    \n    langcheck.metrics.fluency('The black cat is sitting')\n    \n\nTo learn more about the different metrics in LangCheck, see [the Metrics\npage](metrics.html).\n\n\n\n\nUse Cases\n\nSince LangCheck is designed as a library of building blocks, you can easily\nadapt it for various use cases.",
        "factual_consistency_score": 0.9595573345820109
    },
    "ignore": {
        "response_message": "In the land of the rising sun,\nLies a city that's second to none.\nTokyo, oh Tokyo, so bright and so bold,\nA metropolis of stories untold.\n\nFrom the neon lights of Shibuya,\nTo the peaceful gardens of Rikugien,\nTokyo's beauty is beyond compare,\nA city that's always on the mend.\n\nThe food, the culture, the people so kind,\nTokyo is a treasure that's hard to find.\nSo come and visit, and see for yourself,\nThe magic of Tokyo, the city of wealth.",
        "source": "Contributing\n\n\n\n\nContents\n\n  * Installing LangCheck from Source\n  * Running Tests\n  * Documentation\n  * Publishing\n\n\n\n\nContributing\n\nThis page contains instructions for contributing to LangCheck.\n\n\n\n\nInstalling LangCheck from Source\n\nTo install and run the LangCheck package from your local git repo:\n\n    \n    \n    # Install the langcheck package in editable mode with dev dependencies\n    > python -m pip install -e .[dev]\n    # If you are using zsh, make sure to escape the brackets\n    > python -m pip install -e .\\[dev\\]\n    \n    # Try using langcheck\n    # (If you edit the package, just restart the Python REPL to reflect your changes)\n    > python\n    >>> from langcheck.metrics import is_float\n    >>> is_float(['1', '-2', 'a'])\n    Metric: is_float\n    prompt generated_output reference_output  metric_value\n    0   None                1             None             1\n    1   None               -2             None             1\n    2   None                a             None             0\n    \n\n\n\n\nRunning Tests\n\nTo run tests:\n\n    \n    \n    # Run all tests\n    > python -m pytest -s -vv\n    \n    # Run non-optional tests only\n    > python -m pytest -s -vv -m \"not optional\"\n    \n    # Run optional tests only (this requires optional Japanese tokenizers like Mecab)\n    > pip install .[optional]\n    > python -m pytest -s -vv -m \"optional\"\n    \n\n\n\n\nDocumentation\n\nTo make documentation:\n\n  1. **Optional:** Re-generate all `docs/langcheck*.rst` files.\n\n    * `sphinx-apidoc -f --no-toc --separate --module-first -t docs/_templates/ -o docs src/langcheck/ src/langcheck/stats.py`\n\n    * **Warning:** This will overwrite all of our custom text in the `.rst` files, so you must check the code diffs for `UPDATE_AFTER_SPHINX_APIDOC` comments and manually re-apply them.\n\n    * This is only necessary when you add or remove entire packages/modules. If you only edit existing packages/modules, you can skip this step.\n\n    * This only modifies the auto-generated `docs/langcheck*.rst` files (the \u201cAPI Reference\u201d section in the docs). It doesn\u2019t touch the `index.md` and other `.md` or `.rst` files.\n\n    * This uses autodoc to generate `.rst` files at the package/module-level.\n\n  2. Re-generate all `docs/_build/*.html` files from the raw `.rst` and `.md` files.\n\n    * `make -C docs clean html`\n\n    * This uses autodoc to populate .html files at the function-level.\n\n    * Note: you\u2019ll see warnings like \u201cmore than one target found for cross-reference \u2018MetricValue\u2019\u201d. Sphinx seems to get confused when we import a module\u2019s classes into its parent package\u2019s `__init__.py`. This seems to be harmless and there doesn\u2019t seem to be a way to suppress it.\n\n      * \n\n  3. View documentation locally\n\n    * `python -m http.server -d docs/_build/html`\nT\n\n  * tateishi_ono_yamada_reading_ease() (in module langcheck.metrics.ja)\n    * (in module langcheck.metrics.ja.reference_free_text_quality)\n  * threshold (langcheck.metrics.metric_value.MetricValueWithThreshold attribute)\n  * threshold_op (langcheck.metrics.metric_value.MetricValueWithThreshold attribute)\n  * threshold_results (langcheck.metrics.metric_value.MetricValueWithThreshold property)\n  * to_df() (langcheck.metrics.metric_value.MetricValue method)\n    * (langcheck.metrics.metric_value.MetricValueWithThreshold method)\n    * (langcheck.metrics.MetricValue method)\n\n|\n\n  * toxicity() (in module langcheck.metrics)\n    * (in module langcheck.metrics.en)\n    * (in module langcheck.metrics.en.reference_free_text_quality)\n    * (in module langcheck.metrics.ja)\n    * (in module langcheck.metrics.ja.reference_free_text_quality)\n\n  \n---|---  \n  \n\n\n\nV\n\n  * validation_fn() (in module langcheck.metrics)\n    * (in module langcheck.metrics.text_structure)\n\n  \n---  \n  \nBy Citadel AI\n\n\u00a9 Copyright 2023, Citadel AI.",
        "factual_consistency_score": 0.11145009547472
    }
}